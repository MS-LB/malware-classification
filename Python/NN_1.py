# https://www.tensorflow.org/tutorials/structured_data/imbalanced_data

from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
from tensorflow import keras

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#  Pick the file to use
# file_name = '478-dataset-2-attributes'
file_name = '478-dataset-4-attributes'
# file_name = '478-dataset-8-attributes'

# Open the file and read in the data
file_path = 'reduced_dataset/' + file_name + '.csv'
raw_data = pd.read_csv(file_path)
print(raw_data)

print(raw_data[['L1-dcache-loads', 'L1-dcache-stores','class']].describe())

transformed_data = raw_data

# Transforming the data into malware vs non-malware
# Lables for the 'class'  are True (for malware) and False (for benign)
transformed_data['class'] = transformed_data['class'].map({'backdoor': True, 'rootkit':True,'trojan':True,'virus':True,'worm':True, 'benign': False})

print(transformed_data)


neg, pos = np.bincount(transformed_data['class'])
print("np.bincount")
total = neg + pos
print('Examples:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, pos, 100 * pos / total))

# Use a utility from sklearn to split and shuffle our dataset.
train_df, test_df = train_test_split(transformed_data, test_size=0.2)
train_df, val_df = train_test_split(train_df, test_size=0.2)

# Form np arrays of labels and features.
train_labels = np.array(train_df.pop('class'))
bool_train_labels = train_labels != 0
val_labels = np.array(val_df.pop('class'))
test_labels = np.array(test_df.pop('class'))

train_features = np.array(train_df)
val_features = np.array(val_df)
test_features = np.array(test_df)

print("train features:")
print(train_features)
print("\n\n\nval features:")
print(val_features)
print("\n\n\ntest features:")
print(test_features)

scaler = StandardScaler()
train_features = scaler.fit_transform(train_features)

val_features = scaler.transform(val_features)
test_features = scaler.transform(test_features)

train_features = np.clip(train_features, -5, 5)
val_features = np.clip(val_features, -5, 5)
test_features = np.clip(test_features, -5, 5)


print('Training labels shape:', train_labels.shape)
print('Validation labels shape:', val_labels.shape)
print('Test labels shape:', test_labels.shape)

print('Training features shape:', train_features.shape)
print('Validation features shape:', val_features.shape)
print('Test features shape:', test_features.shape)

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'),
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
]

def make_model(metrics = METRICS, output_bias=None):
  if output_bias is not None:
    output_bias = tf.keras.initializers.Constant(output_bias)


  model = keras.Sequential([
      keras.layers.Dense(
          16, activation='relu',
          input_shape=(train_features.shape[-1],)),
      keras.layers.Dense(64),
      keras.layers.Dense(1, activation='sigmoid',
                         bias_initializer=output_bias)
  ])

  model.compile(
      # optimizer=keras.optimizers.Adam(lr=1e-3),
      optimizer=keras.optimizers.Adam(lr=0.001),
      loss=keras.losses.BinaryCrossentropy(),
      metrics=metrics)

  return model



EPOCHS = 100
BATCH_SIZE = 100

# EPOCHS = 20
# BATCH_SIZE = 75

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc',
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

model = make_model()
history = model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks = [early_stopping],
    validation_data=(val_features, val_labels))

colors = ["blue","orange"]

def plot_metrics(history):
  metrics =  ['loss', 'auc', 'precision', 'recall']
  for n, metric in enumerate(metrics):
    name = metric.replace("_"," ").capitalize()
    plt.subplot(2,2,n+1)
    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')
    plt.plot(history.epoch, history.history['val_'+metric],
             color=colors[1], linestyle="--", label='Val')
    plt.xlabel('Epoch')
    plt.ylabel(name)
    if metric == 'loss':
      plt.ylim([0, plt.ylim()[1]])
    elif metric == 'auc':
      plt.ylim([0.8,1])
    else:
      plt.ylim([0,1])

    plt.legend()

# plot_metrics(history)
# plt.show()

train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)
test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)

baseline_results = model.evaluate(test_features, test_labels,
                                  batch_size=BATCH_SIZE, verbose=0)

for name, value in zip(model.metrics_names, baseline_results):
  print(name, ': ', value)
print()

def plot_roc(name, labels, predictions, **kwargs):
  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)

  # todo write fp and tp to a file for tate

  plt.plot(fp, tp, label=name, linewidth=2, **kwargs)
  plt.title('ROC')
  plt.xlabel('False positives')
  plt.ylabel('True positives')
  plt.xlim([-0.01,1])
  plt.ylim([-0.01,1])
  # plt.grid(True)
  # ax = plt.gca()
  # ax.set_aspect('equal')


plot_roc("Train Baseline", train_labels, train_predictions_baseline, color=colors[0])
# plot_roc("Test Baseline", test_labels, test_predictions_baseline, color=colors[1], linestyle='--')
# plt.legend(loc='lower right')
plt.show()

